extends layout.pug
block append stylesheets
  link(rel='stylesheet' defer, href='/stylesheets/bootstrap-grid.css')
  link(rel='stylesheet' defer, href='/stylesheets/bootstrap-reboot.css')
  link(rel='stylesheet' defer, href='/stylesheets/projectBoxes.css')

block append scripts
  script(async, defer, src='/javascripts/video.js')

//-images should be 700x464 to be the same size
block content
      section.project-page-container
        h1.page-header #{title}
        //- p(style="text-align:center;") This page gives an overview of some project I did. Many will be added soon.
        .row
          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='', target="_blank").div-img-top
                //-iframe(width="700" height="400" src="https://www.youtube.com/embed/xG2lreUPnFQ?controls=0&autoplay=1&mute=1&loop=1&modestbranding=0" frameborder="0").card-img-top
                video(autoplay loop controls muted playsinline width="100%").card-img-top
                  source(src="/videos/Planning_video.webm" type="video/webm")
                  source(src="/videos/Planning_video.mp4" type="video/mp4") 
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='https://github.com/AmberElferink/PDM-python-RRT', target="_blank") Drone packet delivery RRT*
                  h5.date part-time, group of 4: 8 weeks - November 2020 to January 2021
                  p.card-text.
                    For the path planning and decision making course, we built a project to simulate dropping of packages on balconies in New York.
                    The visualisation was done with CoppeliaSim (previously V-rep). We linked this to python, and modified an existing RRT* library to speed up significantly.
                    RRT* forms a tree through 3D space by randomly placing a point, and connecting it to the closest point in the tree. It then reconnects some branches to reorganise.
                    We did this by searching in a cone towards the end point. It will first try to fit points in this area, if it cannot find a point closer to the goal
                    within a few iterations, it will enlargen the angle of the cone. In addition to the cone, there is a box around the drone so it can find options when it is close to a building.
                    It then flies over the path by actual drone dynamics with PID velocity control.
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Graded with a 9.1/10
                    li Built in python, linked with Coppeliasim (V-rep)
                    li Full paper available on Github
                <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/PDM-python-RRT', '_blank');"><i class="fa fa-github-square"></i> Checkout Github </button>

          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='https://github.com/AmberElferink/Raytracer/wiki', target="_blank").div-img-top
                //-iframe(width="700" height="400" src="https://www.youtube.com/embed/xG2lreUPnFQ?controls=0&autoplay=1&mute=1&loop=1&modestbranding=0" frameborder="0").card-img-top
                video(autoplay loop controls muted playsinline width="100%").card-img-top
                  source(src="/videos/Animation_engine.webm" type="video/webm")
                  source(src="/videos/Animation_engine.mp4" type="video/mp4") 
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='https://github.com/AmberElferink/AnimationViewer/', target="_blank") Animation engine
                  h5.date part-time, group of 4: 8 weeks - May to June 2020
                  p.card-text.
                    For the Animation course, we built an animation engine from the ground up in C++ and OpenGL. 
                    You first drag in a file containing a rigged mesh, meaning the mesh contains a basic skeletal system with joints. 
                    To then animate it, you drag an animation file on top of the mesh which matches in the number of joints, which makes the mesh move.
                    To correctly animate this, kinematics was implemented throughout the joint system.
                    The animation file is usually loaded in keyframes. To make smooth animations, linear interpolate between te keyframes was implemented.
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Built in OpenGL and C++ from scratch
                    li Intuitive GUI and components
                <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/AnimationViewer/blob/master/README.md', '_blank');"><i class="fa fa-github-square"></i> Checkout Github </button>


          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='', target="_blank").div-img-top
                //-iframe(width="700" height="400" src="https://www.youtube.com/embed/xG2lreUPnFQ?controls=0&autoplay=1&mute=1&loop=1&modestbranding=0" frameborder="0").card-img-top
                video(autoplay loop controls muted playsinline width="100%").card-img-top
                  source(src="/videos/Computer_Vision.webm" type="video/webm")
                  source(src="/videos/Computer_Vision.mp4" type="video/mp4") 
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='', target="_blank") Real world vision projects
                  h5.date part-time, group of 2: 3 weeks - Feb 2020
                  p.card-text.
                    In the computer vision course, we learned how to incorporate simulations into the real world and the world into simulations via cameras.
                    In a first project we used OpenCV to calibrate the camera and find the orientation of the checkboard. Linking this to OpenGL, we draw in the cube.
                    In GLSL fragment shader, reflections to a virtual light source is simulated. 
                    The other assignment was to show 3D reconstruction from four cameras on a prebuilt C++ template. 
                    We extracted a mask of the subject, and turned a voxel (a 3D pixel) on when it was visible from all four cameras. 
                    This way you get a 3D reconstruction in voxels, from which a mesh can be generated via the marching cubes algorithm.
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Cube built in OpenGL, OpenCV and C++ 
                    li 3D reconstruction in OpenCV and C++ (OpenGL already in template)
                    li Both projects graded 10/10
                div
                  <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/ComputerVisionCube', '_blank');"><i class="fa fa-github-square"></i> Checkout Github cube </button>
                  <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/ComputerVisionReconstruction', '_blank');"><i class="fa fa-github-square"></i> Checkout Github 3D </button>
  



          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='#').div-img-top
                video(autoplay loop controls muted playsinline width="100%").card-img-top
                  source(src="/videos/Game-Physics.webm" type="video/webm")
                  source(src="/videos/Game-Physics.mp4" type="video/mp4") 
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='https://github.com/AmberElferink/GamePhysics', target="_blank") Game Physics
                  h5.date part-time, group of 2/3: 8 weeks - March to April 2019
                  p.card-text.
                    In the game physics course in the Game and Media technology master, we implemented rigid body dynamics, soft body dynamics and a ragdoll cloth simulation. 
                    The task was to implement rigid and soft body dynamics in the framework of our teacher, which already could detect collisions, but not solve them. 
                    For the rigid body dynamics, the CR coefficient is how elastic the collision is, the lower, the more energy gets lost. Other built in features: Including friction, air drag, interpenetration with additional rotational movement, interaction with mouse.
                    The soft body simulation works by connecting the vertices via springs in tetrahedral structure. To quickly calculate for all springs simultaneously, the simulation uses matrices for mass, stiffness and damping constants, for which each entry in the matrix matches to a spring.
                    Together with volume preservation forces, this yields a soft body simulation. It allows allow user squeezing and corotational elements 
                    Then we have a cloth and ragdoll simulation. The simulation is built in Unity without RigidBody components (implementing physics ourselves). Due to only two weeks to build it part time, we did not get to angular constraints and inverse kinematics.
                    Cloth simulated as vertices connected by springs. Ragdoll as balls with distance constraints.
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Rigid body (C++), grade 10/10:
                    li Soft body Features (C++), grade 10/10: 
                    li Ragdoll and cloth simulation 
                <button class ="btn-box" ><i class="fa fa-github-square"></i> No Github, copyright </button>  

          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='#').div-img-top
                picture
                  source(srcset="/images/VR-exo.webp" type="image/webp")
                  img.card-img-top(src='/images/VR-exo.png', alt='VR-exo first design image' type="image/png")
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='#') VR-exo, an exoskeleton for Virtual Reality
                  h5.date 3 weeks full-time - December 2018
                  p.card-text
                    | Wat if we were to make an exoskeleton for the entire body, that can be tilted as well?
                    | We would be able to simulate anything in VR. Walk, step onto things, step in a plane and fly away. 
                    | Feel all the forces in a virtual world. Holotron and the AxonVR skeleton are similar to this idea, which I discovered later (see Homepage).
                    | This would also be particulary useful for training, rehabilitation and telerobotics.
                    | I spent a few weeks doing research and designing ideas to solve the locomotion problem with haptic feedback in my spare time.
                    | Since this project I tried to fully focus my education to work towards this goal.
                    | Designing an exoskeleton counteracting human movements is hard due to the high torque and speed requirements for the motors.
                    | Another problem is that you should be able to move without the motor counteracting your forces.
                    | The research I have during this time is downloadable in a PDF below. 
                    | Please keep in mind that during this time I only had chemistry and 1.5 years computer science experience.
                    | Since attending haptics courses, I know what can and cannot be done, and work towards the endgoal differently.
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Done in my spare time.
                    li I researched it in 2018 (see download below).
                    li I am currently studying towards this goal.
                <button class ="btn-box" onclick="window.open('/files/2019-Feb-Vision-of-VRexo-BLDCrevised.pdf', '_blank');"><i class="fa fa-download"></i> Research PDF VR-exo</button>


          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='#').div-img-top
                video(autoplay loop controls muted playsinline width="100%").card-img-top
                  source(src="/videos/exo-arm.webm" type="video/webm")
                  source(src="/videos/exo-arm.mp4" type="video/mp4")              
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='#') VR-exo force feedback elbow
                    h5.date 10 weeks part-time - Feb to April 2019
                  p.card-text
                    | To make a simple proof of concept I started to build a haptic feedback for the elbow joint.
                    | The idea was to have force feedback for instance for shooting a bow in VR.
                    | I never fully finished it, since there were some (solvable) issues with both electronics and welding.
                    | Thereby, I had to start my master which left little time to spend entire days in the workshop.
                    | Since doing my master, I have learned a lot and would redesign most of the exoskeleton.
                    | The above video is only a tiny selection of all the things I worked on. 
              
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Done in my spare time.
                    li Mechanical arm, completed until it only had to be welded.
                    li Torque controller (current sensor and current regulator with H-bridge). Sadly due to a mistake this shorted later.
                    li Old drill for battery and motor, strong enough to counter maximum human force.
                    li Loadcell to measure force.
                    li All should be easily to control from an Arduino.
                <button class ="btn-box" onclick="window.open('/files/2019-Feb-Vision-of-VRexo-BLDCrevised.pdf', '_blank');"><i class="fa fa-download"></i> Research PDF VR-exo</button>

     
          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='https://github.com/AmberElferink/Raytracer/wiki', target="_blank").div-img-top
                //-iframe(width="700" height="400" src="https://www.youtube.com/embed/xG2lreUPnFQ?controls=0&autoplay=1&mute=1&loop=1&modestbranding=0" frameborder="0").card-img-top
                video(autoplay loop controls muted playsinline width="100%").card-img-top
                  source(src="/videos/Raytracer.webm" type="video/webm")
                  source(src="/videos/Raytracer.mp4" type="video/mp4") 
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='https://github.com/AmberElferink/Raytracer/wiki', target="_blank") Ray Tracing and Path Tracing
                  h5.date part-time, group of 2: 3 weeks - May 2018, 8 weeks Nov to Jan 2019
                  p.card-text
                    | A 3D environment that does not have real-time performance as priority, but is physically accurate. This method is mostly used in rendering software.
                    | A ray is shot from each pixel into a virtual space and the intersection with the closests object, bouncing further and its color is determined. 
                    | Some implemented features are: Glass, mirrors, spotlights, anti-aliasing, triangles in C# whitted. The C++ whitted additionally contains Lambert-Beer, textures, area lights and multithreading (205% speedup on dual core h).
                    | The car consists of 360794 triangles. To render it fast, BVH (bounding volume hierarchy is implemented, which is dividing scene in nested boxes, and walking through them with a tree. The car can render on 1280x720px in 180 ms quad core hyperthreaded (no threadpool) and in 2.45 seconds single core. This is very fast for a whitted raytracer on CPU.
                    | Path tracer: Rays are rendered from light source to camera and is capable of caustics and color bleeding, which the other whitted raytracers are not.
                    | To speed this up, I implemented SIMD (AVX) to speed up the triangle intersection function.
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Whitted Raytracers C# (bachelor), graded with 10/10
                    li Whitted C++ (master)
                    li Whitted C++ (master) with BVH, which is many triangles fast
                    li Path tracer (master) which can handle caustics and color bleeding

                <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/Raytracer/wiki', '_blank');"><i class="fa fa-github-square"></i> Checkout Github bachelor </button>
                <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/AdvancedGraphics', '_blank');"><i class="fa fa-github-square"></i> Checkout Github master </button>
        


          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='#').div-img-top
                video(autoplay loop controls muted playsinline width="100%").card-img-top
                  source(src="/videos/Rasterizer.webm" type="video/webm")
                  source(src="/videos/Rasterizer.mp4" type="video/mp4")
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='#') Rasterization Engine
                  h5.date 3 weeks part-time, group of 2 - June 2018
                  p.card-text
                    | A 3D environment that has real-time performance as priority. 
                    | However, shadows, lighting and reflections will be less accurate. This rendering method is mostly used in games.
                    | Implemented many basic engine features from a template that could load a basic 3D meshes.
                    | One of the features implemented is a scenegraph, which enables rotation and translation with respect to parent objects and finally transforming to camera space.
                    | This was done with matrix multiplications.
                    | Other implemented features include: Pong shading, texture mapping and normal maps.
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Built in C# and OpenGL.
                    li Features: Phong shading model, multiple light sources, normal maps and an interactive camera.
                    li Graded with a 9.7/10
                <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/Rasterizer', '_blank');"><i class="fa fa-github-square"></i> Checkout Github</button>
          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='https://github.com/AmberElferink/ImageProcessing3/wiki', target="_blank").div-img-top
                picture
                  source(srcset="/images/handDet.webp" type="image/webp")
                  img.card-img-top(src='/images/handDet.jpg' type="image/jpeg" alt='Hand detection result with beamer projection')  
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='https://github.com/AmberElferink/ImageProcessing3/wiki', target="_blank") Hand detection algorithm
                    h5.date 3 weeks part-time - Nov 2018
                  p.card-text
                    | This algorithm detects the amount of fingers and position of a hand on a table with an overlay projected by a beamer.
                    | This can be used to change the projection when a hand points at a specific spot.
                    | General process:
                    | First, the image is iteratively thresholded to reduce the background noise.
                    | Then, the largest item is isolated. The convex hull and convex defects are generated.
                    | From the angles of the fingers, the amount of fingers and location is determined.
                    | The largest challenges in this project, were the occlusion of beamer light, and the varying arm length and position.       
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Built in C#
                    li Handles occlusion (by beamer light)
                    li Handles rotation of the hand
                    li No external libraries used
                    li Graded with 8.7/10    
                <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/ImageProcessing3/wiki', '_blank');"><i class="fa fa-github-square"></i> Checkout Github </button>
 
          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='https://github.com/AmberElferink/LarryDragonsDen/wiki').div-img-top
                picture
                  source(srcset="/images/Larry.webp" type="image/webp")
                  img.card-img-top(src='/images/Larry.png', alt='Image from story and screenshot from Larry the Unsanitary canary' type="image/png")  
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='https://github.com/AmberElferink/LarryDragonsDen/wiki') Larry the Unsanitary Canary: Dragons Den
                    h5.date 8 weeks part-time - Nov-Dec 2017
                  p.card-text
                    | The game starts when Larry, the trusty steed of the adventures, besmirches the wall of a dragons den. 
                    | The players are locked into the dragons dungeon, and escaping it is their only hope to freedom.
                    | This game was made as a first year introduction project. It was made within two months with a team of seven first year students.
                    | It is a co-op puzzle dungeon crawler, where the players need to find colored keys to unlock doors and each player can only pick up keys of his own color.
                    | Some doors can be opened by levers, which a player can hold down to open doors for eachother.
                    | Along the way, players will encounter evil bunnies and pinguins they must defeat.
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Built in C# and Monogame
                    li Teamleader
                    li Worked mostly on planning, menu's, story art (upper image) and controller input
                    li Personal grade: 8.5/10   
                <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/LarryDragonsDen/wiki', '_blank');"><i class="fa fa-github-square"></i> Checkout Github</button>
            
          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='#').div-img-top
                picture
                  source(srcset="/images/fastdining.webp" type="image/webp")
                  img.card-img-top(src='/images/fastdining.png', alt='Fastdining webshop image' type="image/png")
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='#') Fast Dining Webshop
                    h5.date 3 weeks part-time - March 2017
                  p.card-text
                    | This is a small web shop made for a web technology course.
                    | The main focus was to create a functioning webshop, with both front- and backend. 
                    | Users can make an account, log in, filter products and buy a product whereafter they can view it in their order history.
                    | This project was mostly focused on the back-end and less on responsiveness and looks.
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Users and products are stored in an SQLite database.
                    li It includes: login (sessions), register, update profile, buying products and order history. 
                    li Made using HTML, CSS, Javascript, NodeJs with ExpressJs and an SQLite database.
                    li Graded with 9.75/10
                <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/fastdiningshop', '_blank');"><i class="fa fa-github-square"></i> Checkout Github</button>
 
          .col-lg-4.col-sm-6.portfolio-item
            .card.h-100
              a(href='www.amberelferink.com' target='_blank').div-img-top
                picture
                  source(srcset="/images/Portfolio.webp" type="image/webp")
                  img.card-img-top(src='/images/Portfolio.jpg', alt='Screenshot of the Portfolio Homepage' type="image/jpeg")
              .card-body
                div.upper-content
                  h4.card-title
                    a(href='www.amberelferink.com' target='_blank') Portfolio website
                    h5.date 2 weeks full-time - Feb 2019 + maintenance later
                  p.card-text
                    | This project is the website you are currently visiting. 
                    | It has been built from the ground up in NodeJS with Express as backend, borrowing some elements from bootstrap.
                    | It is a very basic website, mostly front-end oriented.
                    | I could have gone for an amazing template, or a premade wordpress site. 
                    | However, I do not want a designers website, but I do want full control since templates usually do not give much customization options.
                    | I am not a designer or front-end developer, but I like to make projects functional.
                    | It is built to be responsive and simple.
                    | The website was running on a Raspberry pi webserver, which I set up myself. 
                    | Since then, I have moved it to an Oracle Cloud Compute engine VPS.
                div.lower-content
                  h4.skills-title Specifications
                  ul.skills-list
                    li Built using Pug(formally Jade), CSS, NodeJS and Bootstrap
                    li Responsive on all device sizes
                    li Running on a self setup Raspberry webserver
                <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/Portfolio', '_blank');"><i class="fa fa-github-square"></i> Checkout Github</button>
 
          .col-lg-4.col-sm-6.portfolio-item
              .card.h-100
                a(href='https://github.com/AmberElferink/AsteroidsInHaskell' target='_blank').div-img-top
                  video(autoplay loop controls  muted playsinline width="100%").card-img-top
                    source(src="/videos/Asteroids.webm" type="video/webm")
                    source(src="/videos/Asteroids.mp4" type="video/mp4")
                .card-body
                  div.upper-content
                    h4.card-title
                      a(href='https://github.com/AmberElferink/AsteroidsInHaskell' target='_blank') Asteroids in Haskell
                    h5.date 3 weeks part-time - Oct 2018
                    p.card-text
                      | This might seem like a very simple game. However, anyone that has programmed in Haskell can confirm it is a very hard language to work with.
                      | Haskell is a functional programming language, which requires entirely different thinking than the usual imperative programming languages.
                      | Everything in this language works recursively, which means every argument which keeps the game running has to be passed along.
                      | The syntax is very compact and abstract and the code is hard to break down into smaller easy pieces.
                      | Debugging can be a pain, since especially for beginners, the stated errors can be very abstract.
                  div.lower-content
                    h4.skills-title Specifications
                    ul.skills-list
                      li Built in Haskell with the Gloss library
                      li The asteroids are parsed from a json file with the aeson, bytestring and text libraries.
                      li Features parallax scrolling.
                      li Basic enemy AI.
                  <button class ="btn-box" onclick="window.open('https://github.com/AmberElferink/AsteroidsInHaskell', '_blank');"><i class="fa fa-github-square"></i> Checkout Github</button>
          .col-lg-4.col-sm-6.portfolio-item
              .card.h-100
                a(href='').div-img-top
                  picture
                    source(srcset="/images/WaistBag.webp" type="image/webp")
                    img.card-img-top(src='/images/WaistBag.jpg', alt='Leather Waist Bag' type="image/jpeg")
                .card-body
                  div.upper-content
                    h4.card-title
                      a(href='') Leather waist bag
                    h5.date 1.5 week full-time - August 2017
                    p.card-text
                      | Aside from working on computers, I like working with my hands. 
                      | Since pockets for women are usually too small to fit a phone, wallet and keys, I made this to solve that problem.
                      | I wear it on my belt every day.
                      | I first made the design to fit my keys, wallet and phone exactly, and then I added decorations.
                      | I bought vegetable tanned leather, stamped, cut and dyed it, after which I have sewn it together by hand.
                      | I have received a lot of positive feedback on this project and have even been approached by strangers to create a similar bag for them.
                  div.lower-content
                    h4.skills-title Specifications
                    ul.skills-list
                      li Made from vegetable tanned cowhide.
                      li All design and processing steps are done by hand.
                      li High quality, still functional after 3.5 years.